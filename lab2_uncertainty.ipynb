{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPY1-2Q5tCtF"
   },
   "source": [
    "# Lab 2 - uncertainty in neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8BHBn76tCtI"
   },
   "source": [
    "# Plan for today\n",
    "\n",
    "\n",
    "1. Learn about **Expected Calibration Error**\n",
    "    * measure it for an example neural network\n",
    "    * minimize it for that network\n",
    "2. See it's usages in practice:\n",
    "    * out-of-distribution detection\n",
    "    * early exit networks\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1_IXd7-vnZV"
   },
   "source": [
    "**Question 1** - can you think of a real-life example where knowing the prediction uncertainty is important?\n",
    "\n",
    "**YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kEOcjR0stCtK"
   },
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "from torch import nn\n",
    "import torch\n",
    "from typing import List\n",
    "from torchvision.datasets import FashionMNIST, CIFAR10\n",
    "from torchvision import transforms as tv\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.models import vgg16, vgg16_bn, resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crDkmJH8uPyC"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zKPTemJcJg7"
   },
   "source": [
    "Today, we will work with the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset, with a small twist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VmRBEEmHue3G"
   },
   "outputs": [],
   "source": [
    "trans = transform=tv.Compose([tv.ToTensor(), tv.Normalize(mean=[0.5, 0.5, 0.5], std=[1,1,1])])\n",
    "ds = CIFAR10('./data', train=True, target_transform=None, download=True, transform=trans)# transform the data from PIL image to a tensor\n",
    "ds_test = CIFAR10('./data', train=False, target_transform=None, download=True, transform=trans) # transform the data from PIL image to a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQBmBbEucCTP"
   },
   "source": [
    "We will split the data in half, to simulate two data distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gc1vvScmwLo5"
   },
   "outputs": [],
   "source": [
    "# INCLUDED_CLASSES = [0, 2, 4, 6, 8]\n",
    "\n",
    "ds_train_included = Subset(ds, indices = [i for (i, c) in enumerate(ds.targets) if c %2 == 0 ])\n",
    "ds_train_excluded = Subset(ds, indices = [i for (i, c) in enumerate(ds.targets) if c %2 != 0])\n",
    "\n",
    "ds_test_included = Subset(ds_test, indices = [i for (i, c) in enumerate(ds_test.targets) if c %2 == 0 ])\n",
    "ds_test_excluded = Subset(ds_test, indices = [i for (i, c) in enumerate(ds_test.targets) if c %2 != 0 ])\n",
    "\n",
    "batch_size=128\n",
    "\n",
    "dl_train_in = DataLoader(ds_train_included, batch_size, shuffle=True)\n",
    "dl_train_ex = DataLoader(ds_train_excluded, batch_size, shuffle=True)\n",
    "\n",
    "dl_test_in = DataLoader(ds_test_included, batch_size, shuffle=False)\n",
    "dl_test_ex = DataLoader(ds_test_excluded, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Np9c7hriAXr"
   },
   "source": [
    "## 1 - Expected calibration error\n",
    "\n",
    "\n",
    "A perfectly calibrated model will have the following property: **A prediction with score $p$ means that there is a $p$ chance, that the model is right**.\n",
    "\n",
    "\n",
    "Expected Calibration Error (ECE) is a way to measure how well a model estimates **it's own uncertainty**, i.e. how well it is calibrated:\n",
    "\n",
    "To calculate it, we need to:\n",
    "* make predictions with our model\n",
    "* divide the predictions into bins $B_m$ based on their score (confidence)\n",
    "* calculate the accuracy of predictions in each $B_m$\n",
    "* measure the difference between the confidence of each bin and it's mean accuracy\n",
    "\n",
    "Formally:\n",
    "\n",
    "$$\n",
    "ECE = \\sum_{i=1}^M \\frac{|B_m|}{N} \\left| acc(B_m) - conf(B_m) \\right|.\n",
    "$$\n",
    "\n",
    "A useful way to visualize the model calibration is to plot a histogram of prediction scores and a reliability diagram, e.g:\n",
    "\n",
    "![reliability](https://drive.google.com/uc?id=1K1VdWAX1HU5xXbQ-Ya3TTcoGTfjuPM9C)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mdIH1KVqJ1L"
   },
   "source": [
    "\n",
    "Unfortunately, models trained with softmax are not guaranteed to be well-calibrated.\n",
    "\n",
    "### Task for you - calculate ECE and draw a reliability diagram\n",
    "\n",
    "Below, there is a basic neural network definition with configurable depth and an example training loop.\n",
    "\n",
    "Please train this network on CIFAR-10. Afterwards, run predictions on a validation dataset, calculate the ECE and visualize the predictions histogram and the reliabilility diagram (you should produce figures similar to the ones above).\n",
    "\n",
    "You can repeat this for three different network setups by introducing various modifications, e.g.\n",
    "\n",
    "* changing network depth / hidden sizes (the easiest, should be enough to notice something)\n",
    "* adding batchnorm to the network\n",
    "* adding $l2$ regularization\n",
    "\n",
    "Please calculate the ECE and produce visualizations for all three networks. Then, briefly summarize what you did and how it influenced the calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbSHwkTwtCtO"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self, hidden_sizes: List[int], downsize_steps: List[int], in_hw: int = 32, n_classes: int = 10):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      hidden sizes - hidden size of each consecutive convolution\n",
    "      downsize_steps - numbers of convolutions for which there will be stride = 2\n",
    "      in_hw - size of the input image\n",
    "      n_classes - number of output classes\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    assert len(hidden_sizes) >= 1\n",
    "\n",
    "    self.conv1 = nn.Conv2d(3, hidden_sizes[0], 3, padding=1)\n",
    "\n",
    "    convs = []\n",
    "\n",
    "    for i, hs in enumerate(hidden_sizes[:-1]):\n",
    "      hs_next = hidden_sizes[i+1]\n",
    "      stride = 2 if i+1 in downsize_steps else 1\n",
    "      convs.append(\n",
    "          nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hs, hs_next, 3, padding=1, stride=stride),\n",
    "        )\n",
    "      )\n",
    "    self.convs = nn.Sequential(*convs)\n",
    "\n",
    "    dhw = in_hw // (2 ** len(downsize_steps))\n",
    "\n",
    "    out_dim = dhw * dhw * hidden_sizes[-1]\n",
    "    self.out = nn.Sequential(\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(out_dim, n_classes)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "\n",
    "    for i, c in self.convs:\n",
    "      x = c(x)\n",
    "    return self.out(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNAMQigAtCtQ"
   },
   "outputs": [],
   "source": [
    "def train_net(net, number_of_epochs: int = 20, lr: float = 0.001):\n",
    "  net = net.to(device)\n",
    "  loss_fn = nn.CrossEntropyLoss()\n",
    "  opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "  epoch_progress = tqdm(list(range(number_of_epochs)))\n",
    "  for i in epoch_progress:\n",
    "      train_loss = 0\n",
    "      y_train_predicted = []\n",
    "      y_train_true = []\n",
    "      net.train()\n",
    "      for iteration, (X_train, y_train) in tqdm(enumerate(dl_train_in), f\"Training epoch {i}\", total=len(dl_train_in), leave=False):\n",
    "          # notice we are training / testing on dl_train_in - the same distribution of data!\n",
    "          X_train, y_train = [t.to(device) for t in [X_train, y_train]]\n",
    "          opt.zero_grad()\n",
    "          y_pred = net(X_train)\n",
    "          loss = loss_fn(y_pred, y_train)\n",
    "          loss.backward()\n",
    "          opt.step()\n",
    "          train_loss += loss.item()\n",
    "          y_train_predicted.extend(y_pred.argmax(dim=1).cpu().numpy())\n",
    "          y_train_true.extend(y_train.cpu().numpy())\n",
    "\n",
    "\n",
    "      val_loss = 0\n",
    "      y_predicted = []\n",
    "      y_true = []\n",
    "\n",
    "      net.eval()\n",
    "      with torch.no_grad():\n",
    "          for iteration, (X_val, y_val) in tqdm(enumerate(dl_test_in), f\"Val epoch {i}\",  total=len(dl_test_in), leave=False):\n",
    "              X_val, y_val = [t.to(device) for t in [X_val, y_val]]\n",
    "\n",
    "              y_pred = net(X_val)\n",
    "              loss = loss_fn(y_pred, y_val)\n",
    "              val_loss += loss.item()\n",
    "              y_pred = y_pred.argmax(dim=1)\n",
    "              y_true.extend(y_val.cpu().numpy())\n",
    "              y_predicted.extend(y_pred.cpu().numpy())\n",
    "\n",
    "      train_acc = accuracy_score(y_train_true, y_train_predicted)\n",
    "      val_acc = accuracy_score(y_true, y_predicted)\n",
    "      epoch_progress.set_description(f'#Epoch: {i}, train loss: {train_loss:.2f}, train_acc: {train_acc:.2f}, val loss: {val_loss:.2f}, val_acc: {val_acc:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cL9NthintcD6"
   },
   "outputs": [],
   "source": [
    "### caution! In order to get scores between (0,1), you'll need to apply softmax to network outputs!\n",
    "\n",
    "def ece(net, ds_loader: DataLoader) -> float:\n",
    "  ...\n",
    "  ### your code here\n",
    "\n",
    "def draw_scores_histogram(net, ds_loader: DataLoader, title: str=\"\"):\n",
    "  ...\n",
    "  ### your code here\n",
    "\n",
    "\n",
    "def draw_reliability_diagram(net, ds_loader: DataLoader, title: str=\"\"):\n",
    "  ...\n",
    "  ### your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aIJ3E0wn2dVA"
   },
   "outputs": [],
   "source": [
    "net1 = Net([32, 32, 64, 64, 128, 128], [2, 4]) # you may start with this example network\n",
    "\n",
    "net2 = ...\n",
    "net3 = ...\n",
    "\n",
    "for i, net in enumerate([\n",
    "            net1,\n",
    "            net2,\n",
    "            # net3\n",
    "  ]):\n",
    "  print(\"Net\" i)\n",
    "  train_net(net)\n",
    "  draw_reliability_diagram(net, dl_test_in, str(i))\n",
    "  draw_scores_histogram(net, dl_test_in, str(i))\n",
    "  print(ece(net, dl_test_in))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8Kkdtpq20uO"
   },
   "source": [
    "### **Question 2** - please summarize your experiments - what influences the good / bad calibration of the network?\n",
    "\n",
    "**YOUR ANSWER HERE**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAQUoueFbza4"
   },
   "source": [
    "## 2 - Out-of-distribution classification\n",
    "\n",
    "We have seen how the models work with umages of classes the model has learned to recognize.\n",
    "\n",
    "But what if we feed to the network images with the objects the model was not trained on?\n",
    "\n",
    "### Task for you - how does the model behave when processing images with unknown labels?\n",
    "Please draw the score histograms for the `dl_test_ex` dataset (the test dataset with excluded classes) for those three nets.\n",
    "\n",
    "**Question 3** - how do differently calibrated nets react to out-of-distribution images?\n",
    "\n",
    "**YOUR ANSWER HERE**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gEQ8QUHp2w7X"
   },
   "outputs": [],
   "source": [
    "for net in [net1, net2, net3]:\n",
    "  draw_scores_histogram(net, dl_test_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJAVGWjG4OzC"
   },
   "source": [
    "## 3 - Early exit networks\n",
    "\n",
    "One of the interesting use cases of model uncertainty are early exit networks, e.g. [Shallow-Deep Networks (SDN)](https://arxiv.org/abs/1810.07052) or [Zero-Time-Waste](https://arxiv.org/abs/2106.05409):\n",
    "\n",
    "![SDN](https://drive.google.com/uc?id=1REU4cX92utasN3Eix40HMJnhQ2Tmftu6)\n",
    "\n",
    "The basic idea of SDN is to:\n",
    "* pretrain a base network (backbone)\n",
    "* attach linear probes (internal classifiers) to the internal blocks of the backbone and train them (without modifying the backbone further)\n",
    "* during inference, if the certainty of an internal classifier is high enough, output the prediction of that IC without further processing.\n",
    "\n",
    "### Task for you - play with SDN\n",
    "\n",
    "Below there is a modified implementation of the above neural net.\n",
    "\n",
    "**Coding**:\n",
    "* add necessary modifications in order to attach a linear layer to each backbone block and to return outputs from those internal classifiers during the forward pass\n",
    "    * you will need to add modifications to the code of the below network, as well as modify the training loop\n",
    "* train the basic part of the net (without the internal classifiers)\n",
    "* train the internal classifiers **(during this part we shouldn't train the rest of the network!)**\n",
    "\n",
    "**Expected results**:\n",
    "* for each internal classifier, plot the reliability diagram and scores histogram (you should draw similar plots as in section 1 for each IC)\n",
    "* plot the train/test accuracy of each internal classifier. Basically, you should end up with a plot like this:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=15OZD2pPvXIF2c8yG3vUI3FyjFsV1TYlH\" width=\"640\" height=\"400\" allow=\"autoplay\"></img>\n",
    "\n",
    "For the basic net, you may choose one of the previously checked architectures. You don't need to repeat the experiment with different architectures (though you're welcome to!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xERQHmBR3k54"
   },
   "outputs": [],
   "source": [
    "class NetWithHeads(nn.Module):\n",
    "  def __init__(self, hidden_sizes: List[int], downsize_steps: List[int], in_hw: int = 32, n_classes: int = 10):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      hidden sizes - hidden size of each consecutive convolution\n",
    "      downsize_steps - numbers of convolutions for which there will be stride = 2\n",
    "      in_hw - size of the input image\n",
    "      n_classes - number of output classes\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    assert len(hidden_sizes) >= 1\n",
    "\n",
    "    self.conv1 = nn.Conv2d(3, hidden_sizes[0], 3, padding=1)\n",
    "\n",
    "    convs = []\n",
    "    heads = []\n",
    "\n",
    "    for i, hs in enumerate(hidden_sizes[:-1]):\n",
    "      hs_next = hidden_sizes[i+1]\n",
    "      stride = 2 if i+1 in downsize_steps else 1\n",
    "      convs.append(\n",
    "          nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hs, hs_next, 3, padding=1, stride=stride),\n",
    "        )\n",
    "      )\n",
    "      ##### YOUR CODE HERE ####\n",
    "      # for each internal convolution, initialize an internal classifier (a single linear layer)\n",
    "      # which will take the output from this convolution and perform classification (return N classes)\n",
    "      ####\n",
    "\n",
    "\n",
    "    self.convs = nn.Sequential(*convs)\n",
    "\n",
    "    dhw = in_hw // (2 ** len(downsize_steps))\n",
    "\n",
    "    out_dim = dhw * dhw * hidden_sizes[-1]\n",
    "    self.out = nn.Sequential(\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(out_dim, n_classes)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "\n",
    "    ic_outputs = dict()\n",
    "    for i, c in self.convs:\n",
    "      x = c(x)\n",
    "      ##### YOUR CODE HERE ########\n",
    "      # generate the outputs of each internal classifier and add it to\n",
    "      ######\n",
    "    return self.out(x), ic_outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60XShaLHvnZZ"
   },
   "outputs": [],
   "source": [
    "#example initialization of net with 4 downsize steps and 4 internal classifiers\n",
    "net_heads = NetWithHeads([32, 64, 128, 256, 512], [1, 2, 3, 4])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
