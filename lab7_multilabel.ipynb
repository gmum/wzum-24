{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5H2Z31V9qKqc"
   },
   "source": [
    "# Lab 7 - Multilabel Classification\n",
    "\n",
    "Plan for today:\n",
    "\n",
    "* familiarize ourselves with the problem of multilabel classification and metrics for such modelling\n",
    "* implement two multilabel classification methods:\n",
    "  * one-vs-all\n",
    "  * embedding-based\n",
    "\n",
    "**Attention!** We will work with [MS-COCO](https://cocodataset.org/) dataset today. It's pretty large, so we strongly recommend working with Colab today, since it'll be quicker to download the dataset there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CxdayKpsrMKZ"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.models import vgg16, resnet34\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models import vgg16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNsEYGgFv5k7"
   },
   "source": [
    "## 1 - The problem of Multilabel classification\n",
    "\n",
    "In the problem of Multilabel classification, we would like to predict a **set** of labels for a given example.\n",
    "\n",
    "**Question**: why do you think the softmax-based approach is not the best for this scenario?\n",
    "\n",
    "**YOUR ANSWER HERE**\n",
    "\n",
    "Multilabel solutions performance will also be measured a bit differently than in regular classification settings:\n",
    "\n",
    "#### Precision\n",
    "\n",
    "$$\n",
    "  \\frac{|y \\cap \\hat{y}|}{|\\hat{y}|}\n",
    "$$\n",
    "\n",
    "#### Recall \n",
    "\n",
    "$$\n",
    "  \\frac{|y \\cap \\hat{y}|}{|y|}\n",
    "$$\n",
    "\n",
    "#### F1-score\n",
    "* the harmonic average of precision and recall\n",
    "\n",
    "### Task for you - please implement precision and recall for multilabel classification \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sFDQRxYGv_TA"
   },
   "outputs": [],
   "source": [
    "def precision(y_true: torch.Tensor, y_pred: torch.Tensor) -> float:\n",
    "  \"\"\"\n",
    "  Both y_true and y_pred should be tensors of ones and zeros of shape [B, C]\n",
    "  denoting if an object of given class is or isn't present in the photo. \n",
    "  \"\"\"\n",
    "  ...\n",
    "\n",
    "def recall(y_true: torch.Tensor, y_pred: torch.Tensor) -> float:\n",
    "  \"\"\"\n",
    "  Both y_true and y_pred should be tensors of ones and zeros of shape [B, C]\n",
    "  denoting if an object of given class is or isn't present in the photo. \n",
    "  \"\"\"\n",
    "  ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oodfHlT1r2cC"
   },
   "source": [
    "## 2 - Data setup and exploration\n",
    "\n",
    "Today we will work with the [COCO dataset](https://cocodataset.org/#home). Let's download it - in Colab it should take a couple of minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uh5T5Hq4YbLK"
   },
   "outputs": [],
   "source": [
    "!wget http://images.cocodataset.org/zips/train2014.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ojxk4onbZcLi"
   },
   "outputs": [],
   "source": [
    "!unzip train2014.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1ZplyMuesg7"
   },
   "outputs": [],
   "source": [
    "!wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_MMqYQ0euoA"
   },
   "outputs": [],
   "source": [
    "!unzip annotations_trainval2014.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GzsHTOEgYwYU"
   },
   "outputs": [],
   "source": [
    "!wget http://images.cocodataset.org/zips/val2014.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtfW8Xr3Y2wj"
   },
   "outputs": [],
   "source": [
    "!unzip val2014.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fogXx2yBiTPg"
   },
   "outputs": [],
   "source": [
    "with open(\"annotations/instances_train2014.json\") as f:\n",
    "  annot = json.load(f)\n",
    "\n",
    "categories_names = {\n",
    "    c[\"id\"]: c[\"name\"]\n",
    "    for c in annot[\"categories\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QHWRFOduj8H3",
    "outputId": "b6ace75d-8b6e-474f-9727-06e0162ec249"
   },
   "outputs": [],
   "source": [
    "max([an[\"category_id\"] for an in annot[\"annotations\"] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIvCEy7IsNjS"
   },
   "source": [
    "COCO is an object detection dataset. However, today we will be only interested in labels of the objects present in the photos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJL3zUMPkAr2"
   },
   "outputs": [],
   "source": [
    "def instances_to_multilabel_vector(instances: List[dict], vector_size: int = 91):\n",
    "  vector = torch.zeros(vector_size)\n",
    "  for i in instances:\n",
    "    vector[i[\"category_id\"]] = 1\n",
    "  return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4oKkPU1OlIv_",
    "outputId": "1c3804b1-fccc-45d5-88f3-2c7d984d3733"
   },
   "outputs": [],
   "source": [
    "train_set = CocoDetection(\n",
    "    root=\"train2014\", annFile=\"annotations/instances_train2014.json\",\n",
    "    target_transform=instances_to_multilabel_vector,\n",
    "    transform=T.Compose([\n",
    "            T.Resize(224),\n",
    "            # Feel free to add augmentation!\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(\n",
    "              mean=[0.485, 0.456, 0.406],\n",
    "              std=[0.229, 0.224, 0.225],\n",
    "          )\n",
    "      ])\n",
    "    )\n",
    "\n",
    "val_set = CocoDetection(\n",
    "    root=\"val2014\", annFile=\"annotations/instances_val2014.json\",\n",
    "    target_transform=instances_to_multilabel_vector,\n",
    "    transform=T.Compose([\n",
    "            T.Resize(224),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(\n",
    "              mean=[0.485, 0.456, 0.406],\n",
    "              std=[0.229, 0.224, 0.225],\n",
    "          )\n",
    "      ])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_zrKpB4ta8P"
   },
   "source": [
    "Let's examine the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-XyuF83PlyJE",
    "outputId": "a5c39897-d055-49b9-96af-8b19a95eea5a"
   },
   "outputs": [],
   "source": [
    "for i, (X, y) in enumerate(train_set):\n",
    "\n",
    "  plt.imshow(\n",
    "      X.permute(1,2,0)\n",
    "      # we're dealing with images transformed to tensors \n",
    "      # so in order to view them we need to permute them.\n",
    "  )\n",
    "\n",
    "  cats = [f\"{i}:{categories_names[i]}\" for i, oh in enumerate(y) if oh==1]\n",
    "  plt.title(\" \".join(cats))\n",
    "  plt.show()\n",
    "\n",
    "  if i > 5:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-gRi_GgvOIB"
   },
   "source": [
    "When working with multilabel problems, it is often useful to analyze the data and see which labels tend to co-appear with one another. \n",
    "\n",
    "For example, humans may commonly appear together with dogs, but girraffes are unlikely to co-appear with umbrellas.\n",
    "\n",
    "### Task for you - analyze the co-existence of labels\n",
    "\n",
    "* go through the training dataset\n",
    "* count the number of co-appearances of each **pair** of labels\n",
    "* print the top-10 most commonly co-appearing label pairs, along with their class IDs and names (names are provided in the `categories_names` dict)\n",
    "* you can also draw a heatmap of all co-occurences\n",
    "* **note** - please avoid redundancy - e.g. if you count appearances of the `(A, B)` pair, you don't need to also count appearances of the `(B, A)` pair.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2KY5C-BGRmn7"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjbznNRip-Ug"
   },
   "source": [
    "## 2 - One-vs-all classification\n",
    "\n",
    "One way to approach multi-label classification is to treat it as $N$ binary classifications - take a model which outputs $N$ labels, but instead of applying $softmax$ as the final activation, applies $sigmoid$ function.\n",
    "\n",
    "We can train such models with the [BCELoss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html).\n",
    "\n",
    "### Task for you - train a model with one-vs-all method\n",
    "* report the previously implemented precision and recall on the validation set throughout the training\n",
    "* note that the grond-truth label vectors and output label vectors must hve the same dimentionality (91).\n",
    "* note that precision and recall expect the vectors of `0`'s and `1`'s - therefore, when calculating those metrics, you need to transform the sigmoid output to such format!\n",
    "\n",
    "**Hint**:\n",
    "\n",
    "In order to simplify the training, you can take a pre-trained model from torchvision, e.g. VGG or ResNet and replace it's final layer with a linear layer with the appropriate number of classes (91), followed by sigmoid. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_p-oGoONl9ck"
   },
   "outputs": [],
   "source": [
    "model = vgg16(pretrained=True)\n",
    "model.classifier[-1] = nn.Sequential(\n",
    "    nn.Linear(model.classifier[-1].in_features, 91),\n",
    "    nn.Sigmoid()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQTTEWlijGGL"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE WITH THE TRAINING LOOP SHOULD GO HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_QSltrikmBc"
   },
   "source": [
    "## 3 - Taking advantage of class correlations\n",
    "\n",
    "The sigmoid classifies each label **separately**. Even though it's internal features may in practice learn to associate two labels together, we can force it to do this explicitly.\n",
    "\n",
    "For this approach we'll need 3 networks:\n",
    "* $F_X$ - transforms images $x$ into embeddings $E_X$\n",
    "* $F_Y$ - transforms label vectors $y$ into embeddings $E_Y$\n",
    "* $F_D$ - transforms embeddings $E_X$ into predictions $Y_p$\n",
    "\n",
    "The embeddings $E_X$ and $E_Y$ are of equal latent dimentionality $l$, possibly lower than $c$ (number of classes).\n",
    "\n",
    "\n",
    "The models will optimize the sum of three loss functions:\n",
    "\n",
    "1. We want the embeddings of pictures with a set of labels to be similiar to the embeddings of that set of labels: \n",
    "$$\n",
    "L1 = \\|F_X(x) - F_Y(y) \\|^2 =\\\\ \n",
    "\\|E_X - E_Y\\|^2\n",
    "$$\n",
    "\n",
    "2. We want the embeddings to be ortonormal (lengths of 1 and each pair of embedding vectors should be orthogonal to its transposed version). Similarly to what happens in self-supervised learning, this prevents the representations from collapsing due to $L1$.\n",
    "\n",
    "$$\n",
    "L2 = \\|F_X(X)F_X(X)^T - I\\|^2 + \\|F_Y(Y)F_Y(Y)^T - I\\|^2  = \\\\\n",
    "\\|E_X E_X^T - I \\|^2 + \\|E_Y E_Y^T - I \\|^2\n",
    "$$\n",
    "\n",
    "where $I \\in \\mathbb{R}^{l \\times l} $ (see Eq. 2 of [this paper](https://ojs.aaai.org/index.php/AAAI/article/view/10769/10628) ).\n",
    "\n",
    "3. We want to decode the predicted labels from the embeddings:\n",
    "\n",
    "$$\n",
    "L3 = \\| F_D(F_X(x)) - y \\|^2 = \\\\\n",
    "\\| F_D(E_X) - y \\|^2\n",
    "$$\n",
    "\n",
    "By minimizing $L1 + L2$, we obtain **C**anonical **C**orrelation **A**nalyis. Minimizing $L3$ lets us decode the obtained embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VdiiwZZPMf1"
   },
   "source": [
    "### Task for you - train a model with the embedding-based method\n",
    "\n",
    "* implement $L1, L2, L3$ losses and the training loop\n",
    "* report the previously implemented precision and recall on the validation set throughout the training\n",
    "* for obtaining embeddings, use the same kind of model as before, but replace the final layer with linear layer with your chosen embedding size\n",
    "* networks $F_Y$ and $F_D$ should be sequences of a couple of linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TewM7mhmPyYT"
   },
   "outputs": [],
   "source": [
    "def l1(e_x, e_y):\n",
    "  \"\"\"\n",
    "  e_x, e_y: tensors of shape [b, embedding_size]\n",
    "  \"\"\"\n",
    "\n",
    "  # YOUR CODE HERE\n",
    "\n",
    "def l2(e_x, e_y):\n",
    "\n",
    "  # YOUR CODE HERE\n",
    "\n",
    "def l3(y_true, y_pred):\n",
    "  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DVyhkRm5kjkN"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE WITH THE TRAINING LOOP SHOULD GO HERE"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "wzum_lab6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
