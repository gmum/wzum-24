{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFPJebZLiTr-"
   },
   "source": [
    "# Lab 7 - few-shot learning and hypernetworks\n",
    "\n",
    "Plan for today:\n",
    "* learn about the concept of few-shot learning\n",
    "* familiarize ourselves with hypernetworks\n",
    "* connect those two concepts by implementing a technique from [this paper](https://arxiv.org/pdf/1706.03466.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HkUkXXVTkSUy",
    "outputId": "0c9849f8-bfa1-4804-e677-47b9264953dc"
   },
   "outputs": [],
   "source": [
    "# !pip install learn2learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "A5seZO8iiXtY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import Omniglot, EMNIST\n",
    "from torchvision import transforms as T\n",
    "import learn2learn as l2l\n",
    "import matplotlib.pyplot as plt\n",
    "from learn2learn.data import MetaDataset, TaskDataset\n",
    "from learn2learn.vision.models import OmniglotCNN\n",
    "from torch import nn\n",
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WT2nPXZNdouR"
   },
   "source": [
    "\n",
    "## Few-shot learning\n",
    "\n",
    "In general, neural networks require huge amounts of data to train well. Few-shot learning techniques aim to construct models, which are capable of quickly adapting to numerous **tasks** based on limited amounts of data.\n",
    "\n",
    "One of the most popular usecases for FSL is image classification. We define $K$-shot, $N$-way classification as the task of classifying between $N$ classes based on $K$ examples for each of the classes, called the **support set**. The model is then tasked with classifying the **query set** of previously unseen images, which belong to the same set of $N$ classes.\n",
    "\n",
    "During training, we construct **tasks** consisting of support and query examples from a set of training classes and taks the model with adapting to those tasks.\n",
    "\n",
    "We evaluate the model on tasks sampled from a set of classes **separate from the training set** - after all, we want to measure how well the model adapts to previously unseen tasks!\n",
    "\n",
    "\n",
    "\n",
    "One of the most popular datasets for FSL is Omniglot. \n",
    "\n",
    "### Task for you - import the omniglot dataset from the [learn2learn](http://learn2learn.net/) package and visualize an example task:\n",
    "* sample a single task from the tasksets\n",
    "* draw a grid with images and their classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ez1HC-Cpi3ZV",
    "outputId": "ef13bb9d-b314-4105-cd94-3bd262858a34"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mprzewie/.anaconda3/envs/uj/lib/python3.8/site-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/brendenlake/omniglot/master/python/images_background.zip to /home/mprzewie/data/omniglot-py/images_background.zip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ec76a2315c4f7d8e5b3fa9acb0db32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9464212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/mprzewie/data/omniglot-py/images_background.zip to /home/mprzewie/data/omniglot-py\n",
      "Downloading https://raw.githubusercontent.com/brendenlake/omniglot/master/python/images_evaluation.zip to /home/mprzewie/data/omniglot-py/images_evaluation.zip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcac7d83ba5a49e49f88923a978bad4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6462886 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/mprzewie/data/omniglot-py/images_evaluation.zip to /home/mprzewie/data/omniglot-py\n"
     ]
    }
   ],
   "source": [
    "shots = 5\n",
    "queries = 15\n",
    "ways = 5\n",
    "\n",
    "\n",
    "tasksets = l2l.vision.benchmarks.get_tasksets('omniglot',\n",
    "                                                  train_ways=ways,\n",
    "                                                  train_samples=shots + queries,\n",
    "                                                  test_ways=ways,\n",
    "                                                  test_samples=shots + queries,\n",
    "                                                  num_tasks=20000,\n",
    "                                                  root='~/data',\n",
    "                                              \n",
    "    )\n",
    "\n",
    "for X, y in tasksets.train:\n",
    "  break\n",
    "\n",
    "# your code here - visualize the examples from X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jO4fqMv2i4JM"
   },
   "source": [
    "## Hypernetworks\n",
    "\n",
    "Hypernetworks are models which, based on some condition, predict weights of other neural networks, which perform the downstream tasks. The concept has been utilized in many fields, such as generative models, point clouds, condtional flows, as well as few-shot learning.\n",
    "\n",
    "\n",
    "## Bringing it all together - Parameter Prediction from Activations\n",
    "\n",
    "Today, we will utilize the hypernets in the task of FSL. We will base our solution on the [Few-Shot Image Recognition by Predicting Parameters from Activations](https://arxiv.org/pdf/1706.03466.pdf).\n",
    "\n",
    "The PPA model consists of:\n",
    "* a convolutional backbone\n",
    "* a parameter prediction hypernet\n",
    "\n",
    "First, we process the support and query samples through the backbone and obtain embeddings $E$ Next, we want to predict the weights of the classifier which will transform $E$ into classes $C$. The classifier is therefore a linear layer with dimentionality $(E, C)$.\n",
    "\n",
    "We can predict the weights of the classifier in several ways:\n",
    "* concatenate all of the support embeddings and predict all of the classifier parameters\n",
    "* predict the *portion* of parameters of shape $(E, 1)$ dedicated to predicting class $C$ based only on the support embeddings from that class. Then, concatenate all portions into weights of shape $(E, C)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbStdIILYr_r"
   },
   "source": [
    "### Task for you - implement the few-shot hypernetwork\n",
    "* implement two variants of classifier generation:\n",
    "  * generating **all** weights based on **all** support class embeddings\n",
    "  * generating **weight fragments** responsible for predicting class $C$ based solely on support embbedings of class $C$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UTGOmMmVQ_ds"
   },
   "outputs": [],
   "source": [
    "class Hypernet(nn.Module):\n",
    "  def __init__(self, n_shot: int, n_way: int, hidden_size: int = 64, weights_per_class: bool = True):\n",
    "    super().__init()\n",
    "    self.cnn = OmniglotCNN(hidden_size=hidden_size).features \n",
    "    # a convolutional net which transforms an image of shape (1, 28, 28) to vectors of shape `hidden_size`\n",
    "\n",
    "    self.weight_predictor = ... \n",
    "    # a neural network that tranforms vectors of shape [n_classes, n_shots, hidden_size] into the weights of a classifier\n",
    "\n",
    "  def forward(\n",
    "      self, \n",
    "      support_examples: torch.Tensor, \n",
    "      support_labels: torch.Tensor,\n",
    "      query_examples: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    support_examples: [n_shot * n_way, 1, 28, 28]\n",
    "    support_labels: [n_shot * n_way]\n",
    "    query_samples: [n_query]\n",
    "\n",
    "    Returns a tuple of logits:\n",
    "      (y_pred_support, y_pred_query)\n",
    "      of shapes:\n",
    "      (\n",
    "        [n_shot * n_way, n_way],\n",
    "        [n_query, n_way]        \n",
    "      ) \n",
    "    \"\"\"\n",
    "\n",
    "    # 1: process the supports and queries through the cnn\n",
    "    # 2: generate the weights of the classifier based on the support embeddings\n",
    "    # 3: classify the support and query embeddings with the generated weights\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xIeWSusbW4f_"
   },
   "outputs": [],
   "source": [
    "shots = 5\n",
    "queries = 15\n",
    "ways = 5\n",
    "\n",
    "\n",
    "tasksets = l2l.vision.benchmarks.get_tasksets('omniglot',\n",
    "                                                  train_ways=ways,\n",
    "                                                  train_samples=shots + queries,\n",
    "                                                  test_ways=ways,\n",
    "                                                  test_samples=shots + queries,\n",
    "                                                  num_tasks=20000,\n",
    "                                                  root='~/data',\n",
    "                                              \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pi1Jsaz8YFAz"
   },
   "source": [
    "### Task for you - finish implementing the training loop:\n",
    "* add the necessary training loss and optimizer parts\n",
    "* track the meta-training and meta-validation losses and accuracies throughout the training epochs and plot them after the training\n",
    "* train the two variants of hypernetwork on Omniglot \n",
    "* train the hypernets in two settings:\n",
    "  * 1-shot, 5-way\n",
    "  * 5-shot, 5-way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2S1o2wd0Q3_-"
   },
   "outputs": [],
   "source": [
    "def train_hypernet(\n",
    "    hypernet: Hypernet,\n",
    "    tasksets,\n",
    "    optimizer,\n",
    "    num_epochs: int = 20,\n",
    "    n_shot: int = shots,\n",
    "    n_query: int = queries,\n",
    "    n_ways: int = ways,\n",
    "    img_shape = (1, 28, 28)\n",
    "):\n",
    "\n",
    "  for e in range(num_epochs):\n",
    "    # meta-training:\n",
    "\n",
    "    for X, y in tasksets.train:\n",
    "      # reshape X and y to have each class in a separate row\n",
    "      X = X.reshape(n_ways, n_shot+n_query, *img_shape)\n",
    "      y = y.reshape(n_ways, n_shot+n_query,)\n",
    "\n",
    "      # separate support from query\n",
    "      X_support, X_query = X[:, :n_shot], X[:, n_shot:]\n",
    "      y_support, y_query = y[:, :n_shot], y[: ,n_shot:]\n",
    "\n",
    "      # re-flatten the tensors\n",
    "      X_support = X_support.reshape(n_ways * n_shot, *img_shape)\n",
    "      X_query = X_query.reshape(n_ways * n_query, *img_shape)\n",
    "      y_support = y_support.reshape(n_ways * n_shot)\n",
    "      y_query = y_query.reshape(n_ways * n_query)\n",
    "\n",
    "\n",
    "      # predictions\n",
    "      y_support_pred, y_query_pred = hypernet(X_support, y_support, X_query)\n",
    "\n",
    "      # YOUR CODE HERE\n",
    "      # \n",
    "      #####\n",
    "      \n",
    "\n",
    "\n",
    "    # meta-validation\n",
    "    for X, y in tasksets.train:\n",
    "      # reshape X and y to have each class in a separate row\n",
    "      X = X.reshape(n_ways, n_shot+n_query, *img_shape)\n",
    "      y = y.reshape(n_ways, n_shot+n_query,)\n",
    "\n",
    "      # separate support from query\n",
    "      X_support, X_query = X[:, :n_shot], X[:, n_shot:]\n",
    "      y_support, y_query = y[:, :n_shot], y[: ,n_shot:]\n",
    "\n",
    "      # re-flatten the tensors\n",
    "      X_support = X_support.reshape(n_ways * n_shot, *img_shape)\n",
    "      X_query = X_query.reshape(n_ways * n_query, *img_shape)\n",
    "      y_support = y_support.reshape(n_ways * n_shot)\n",
    "      y_query = y_query.reshape(n_ways * n_query)\n",
    "\n",
    "      # YOUR CODE HERE\n",
    "      # \n",
    "      ####\n",
    "\n",
    "  # plot the training / validation losses and accuracies\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQL1gfVol0x_"
   },
   "outputs": [],
   "source": [
    "# initialize and train the hypernetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0OM96Qfln-U"
   },
   "source": [
    "**Question for you** - which variant of the few-shot hypernetwork worked better? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdW7nq_9loDo"
   },
   "source": [
    "## Final validation\n",
    "\n",
    "Let's validate our models on one more dataset - EMNIST - which contains digits and latin alphabet characters\n",
    "\n",
    "### Task for you\n",
    "* based on [documentation](http://learn2learn.net/tutorials/task_transform_tutorial/transform_tutorial/), prepare the EMNIST meta-dataset. Then, calculate the accuracy of the hypernetworks you've trained on the tasks from that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GV2m_kSGnlPm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-ozSaJum1Aj"
   },
   "source": [
    "# From 28.05 - project presentations!\n",
    "* Guidelines are [here](https://docs.google.com/document/d/1Xr49OjhKMTZu1Cxmz3b1exezXf9OWDgnlo3IzuYEhtw/edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "wzum_lab7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "uj",
   "language": "python",
   "name": "uj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
